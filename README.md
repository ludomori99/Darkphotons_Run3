# General information

Framework to perform CMS Dark Photon analysis in Run3.

Pipeline for data extraction, selection, computation of systematics and computation of limits is to be found in `pipelines/extraction_pipeline`. Ideally one can customize the pipeline by choosing a given configuration file. For now it is enough to use one single configuration file; if one wants to be able to use multiple, it is enough to change the path of the yaml file in the `load_analysis_config()` functions at the head of every file to be defined as an environment variable in the pipeline script. Some things are hardcoded (for now) for practicality as long as the framework is under construction.

To run every piece of the analysis it is convenient to simply execute the command `source extraction_pipeline.sh`, after having uncommented the part one is interested in. It is good to do a safety check that the script are doing what is expected by controlling the `main` at the bottom of every script. 
 
Main steps are:
* extraction of data. Task defined in `pull_data/offline/run_merge_off.sh` and submitted to distributed computing system using Condor with the command `condor_submit $DPLUDO/pull_data/offline/condor_off.sub`. Script templates are constructed based on employed configuration by executing `python3 $DPLUDO/utils/generate_scripts_extraction.py $test_or_depl`. `$test_or_depl` is (for now) the pipeline variable that encodes the choice of configuration. Configurable # of jobs (files) delivers output in `<DATA>/DimuonTrees/offline/dump/`, and is then prepared for training by application of mass cuts using `python3 $DPLUDO/utils/skim_mass.py $test_or_depl`
* extraction of MC, ibid. In this case use Slurm (as long as MC is stored on SubMIT), entirely analog as data. Need a final script bash `$DPLUDO/pull_data/MC/merge_final.sh` to merge the distributed outputs. 
* BDT training to select DY-like dimuon events from known meson resonances. Based off XGBoost, Model name includes method (forest/tree/...) and meson used for training. `BDT/evaluate_BDT.py` will add the MVA score, under a branch with the name of the BDT model, to the ntuple file.
* Viewing/evaluating the BDT is done using notebooks. Built to be used within or without pipeline, only to produce plots (least possible processing). Evaluation takes place on data and MC to estimate systematic uncertainty.  

# Extracting data
Condor files and bash scripts are generated in the file utils/generate_scripts_extraction.py, and final scripts should not be edited directly. Slurm files (in python) are directly to be edited in the pull_data/... folder, since can integrate with configuration file easily. Only basic assumptions on the structure of the file system are made (e.g. one should not change the name of the directories in the cloned repository.) Paths are defined in the configuration. The key scripts that build and fill the root files are `generateXXXDimuonTree.C`, with `XXX\in{'Off','MC'}`. We may want to shift to RooDataFrames just for practicality, since adding branches is somewhat cumbersome. In the script we apply a preselection on the $p_T$ and $\eta$, require opposite charge, and choose the candidate dimuon whose individual muons have the highest $p_T$. In the MinBias MC samples we also apply a PDGId condition, corresponding to the resonance events we want to select. There is a small monitoring script to scan the logs of the condor and slurm jobs, searching for critical keywords. It is by no means a definitive check, and also not automated to learn whether the jobs are done yet. Data is delivered to "dump" folders, with many files. Offline "dump" data  needs to be skimmed and merged, depending on the purpose. To proceed with the study and MVA training on the resonances, we use `skim_mass.py` to apply a cut in the mass, as indicated in the configuration (search for "inclusive"), and produce the `merged_X.root` file, where `X` is meant to indicate the ntuple scheme (for now used just `X=A`). For practicality, only a subset of events is taken. When generating the ntuples, an argument of the C++ script is `event_fraction`, and at skimming level we sample randomly a fraction given in the `config`. `skim_mass.py` also enables to create Tag'n Probe samples, in a similar fashion. 

# Training of BDT
This section mainly refers to the script `BDT/training_offline.py`, where the `Trainer` object is defined, as well as the `BDT/view_training.ipynb` notebook, used to visualize the traning results, and the `BDT/evaluate_BDT.py` script, used to extend the ntuples by the BDT score of a given model. The training is performed with the python package XGBoost. Different models are trained, used, and compared, based on different general techniques, hyperparameters, input variables and data used for training. The general syntax for defining a trained network is `<model name>_<particle used for training>`, e.g. `forest_standard_Jpsi` or `forest_ID_Y`. The particle defines the data we are using for training, whereby we take signal to be a mass band close to the meson resonance peak(s), and background as sidebands just outside the resonant region. The bands are defined in the configuration file `config/analysis_config.yml`, as well as each model's (hyper)parameters. The structure of the configuration file should ideally be self-explanatory, or for more information one can look at https://xgboost.readthedocs.io/en/stable/. It should be noticed that to attain significant acceleration of training, XGBoost supports a CUDA interface. For training with current configuration (in particular with `device='cuda'`) need to have a working GPU on the used machine. To check whether there are available GPUS and the CUDA version is compatible, one can execute `nvidia-smi` from terminal (CUDA 11.0, Compute Capability 5.0 required, https://xgboost.readthedocs.io/en/stable/gpu/index.html).

The most straightforward way to train a model (say `forest_ID`) on data from a particle (say `Y`) is by including the following lines in the `main` of `training_offline.py`:
```
Y_trainer = Trainer("Y", 'forest_ID')
Y_trainer.complete_train()
Y_trainer.plot_model() #if desired, can provide kwarg saveas=config["locations"]["public_html"]+"BDTs/Y_forest_standard.png"
```

The `complete_train` function executes in sequence `load_data`, which imports to the `Trainer` object the ntuple for the particle we are interested in, based on info given in initialization. For the purpose of comparison plots, it is sometimes good to add the flag `include_MC=True` to the kwargs of `load_data`, which is by default off. The next step is `prepare_training_set`, which will extract the mass cuts that define signal and background from the `config`. In case the signal consists of multiple intervals (e.g. for multiple Y peaks) one needs to give as kwarg `signal_indices` a subarray of `[1,2,3]`. At evaluating stage, it can be useful to use a particle data for evaluation that is different from the training particle (e.g. see result of evaluating on Y the BDT which was trained on Jpsi), to do so one can "override" the particle by providing the data in the desired mass range as argument `data_override`, and providing the string name of the particle as `data_particle`, which will provide the intervals that define signal region and sidebands background. The fractions of data to be used for training, validating the hyperparameters and testing are hardcoded in the `train_test_split` calls from scikit, given the degree of dependence from the luminosity of the initial ntuple. The samples are large enough to have the risk of overtraining the hyperparameters very low, hence the testing sample is small and will be used only in the end to obtain the efficiency, once the selection strategy is settled. The size of the signal and sideband samples are compared and events are thrown away to have the two sets be of the same size. There is the possibility to add weights at training stage, by using the `weights` kwarg. 

For the Jpsi prompt analysis we need to apply reweighing to the Jpsi data. For this purpose we define the `train_prompt_Jpsi()` function separately. We first compute the reweighing on the prompt center of the Jpsi peak, and define the weight of the sideband background events to be constant. At training stage, the background weights are tuned by a multiplicative factor `w_frac_bkg` in the function `prepare_training_set`. We also added some small scripts to add a `prompt_weight` branch to each ntuple we are using, for the case we are not training on that data. This is just to keep the ntuples consistent when we are to work with them to extract the systematics. 

To display the results of the training, a useful script is the `view_training.ipynb` notebook, which is meant to be self-explanatory.  


**Hardcoded parts**
Everything else (except for minor and not relevant stuff) should be only modified from the configuration file. 
- Set of variables stored in the ntuples. Can be changed in `pull_data/MCRun3/generateMCDimuonTree.C` and `pull_data/offline/generateOffDimuonTree.C`. They must necessarily be a superset of the variables used for training and selection as defined in the `config`. Current choice from extraction process is hardcoded to result in skimmed merged datasets defined as `merged_A.root:tree`. Will generalize when needed. 
- Paths of files in sPlot.C. For time being need to change manually. In future might integrate the config to be read by the C++ script as well.
- step size (now 10000) of event chunks when evaluating bdt on offline in evaluate_BDT.py. Probably no need to put in config
